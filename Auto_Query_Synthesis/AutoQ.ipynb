{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f274c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\NLP\\GraphRag_Eval\\autoq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f395b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17a47e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from pydantic import SecretStr\n",
    "\n",
    "from benchmark_qed.autod.data_processor.embedding import TextEmbedder\n",
    "from benchmark_qed.autod.io.text_unit import load_text_units\n",
    "from benchmark_qed.autoq.io.activity import (\n",
    "    save_activity_context,\n",
    ")\n",
    "from benchmark_qed.autoq.io.question import (\n",
    "    load_questions,\n",
    "    save_questions,\n",
    ")\n",
    "from benchmark_qed.config.llm_config import LLMConfig, LLMProvider\n",
    "from benchmark_qed.llm.factory import ModelFactory\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "if logging.getLogger(\"httpx\") is not None:\n",
    "    logging.getLogger(\"httpx\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2477c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CONFIGS\n",
    "INPUT_DATA_PATH = \"/data\"\n",
    "OUTPUT_DATA_PATH = \"autoq/processed_data\"\n",
    "OUTPUT_QUESTIONS_PATH = \"autoq/questions\"\n",
    "TEXT_COLUMN = \"body_nitf\"\n",
    "METADATA_COLUMNS = [\"headline\", \"firstcreated\"] # Ap news Dataset\n",
    "FILE_ENCODING = \"utf-8-sig\"\n",
    "\n",
    "# tokenizer used for chunking documents into text units\n",
    "ENCODING_MODEL = \"o200k_base\"\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# DATA SAMPLING CONFIGS\n",
    "# These configs control the breadth and depth of the selected data sample.\n",
    "# Adjust these parameters based on your data size and the number of questions to be generated (e.g. try increasing number of clusters if you want to generate more diverse questions)\n",
    "# The final sample size will be NUM_CLUSTERS * NUM_SAMPLES_PER_CLUSTER\n",
    "NUM_CLUSTERS = 10\n",
    "NUM_SAMPLES_PER_CLUSTER = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# GENERAL QUESTION GENERATION CONFIGS\n",
    "# Number of questions to generate for each question class. You can also specify a different number of questions for each class.\n",
    "NUM_QUESTIONS = 2\n",
    "# Factor by which to overgenerate candidate questions (you can specify a different factor for each question class). These candidate questions will be ranked and filtered using a question sampler to select the final questions.\n",
    "OVERSAMPLE_FACTOR = 2.0\n",
    "\n",
    "# CONFIGS SPECIFIC TO ACTIVITY QUESTIONS\n",
    "# these configs should be adjusted based on the number of questions to be generated. Try increasing these configs if you want to generate more questions.\n",
    "NUM_PERSONAS = 5\n",
    "NUM_TASKS_PER_PERSONA = 2\n",
    "NUM_ENTITIES_PER_TASK = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "997f0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CONFIGS\n",
    "API_KEY = SecretStr(os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "LLM_MODEL = \"gpt-4.1-nano\"\n",
    "LLM_PARAMS = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"seed\": 42,\n",
    "}  # adjust this based on your model. For example, some reasoning models do not support temperature settings\n",
    "CONCURRENT_REQUESTS = (\n",
    "    8  # Control for request concurrency. Adjust this based on your model capacity.\n",
    ")\n",
    "\n",
    "text_embedder = TextEmbedder(\n",
    "    ModelFactory.create_embedding_model(\n",
    "        LLMConfig(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            api_key=API_KEY,\n",
    "            llm_provider=LLMProvider.OpenAIEmbedding,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "llm = ModelFactory.create_chat_model(\n",
    "    model_config=LLMConfig(\n",
    "        model=LLM_MODEL,\n",
    "        api_key=API_KEY,\n",
    "        llm_provider=LLMProvider.OpenAIChat,\n",
    "        call_args=LLM_PARAMS,\n",
    "    )\n",
    ")\n",
    "token_encoder = tiktoken.get_encoding(ENCODING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad0bf6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:benchmark_qed.autod.sampler.sample_gen:Document count: 1083\n",
      "INFO:benchmark_qed.autod.sampler.sample_gen:Text unit count: 2851\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\shash\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\shash\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:benchmark_qed.autod.sampler.clustering.kmeans:Cluster sizes: min=104, max=516, mean=285.1\n",
      "INFO:benchmark_qed.autod.sampler.sample_gen:Sampled text unit count: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 100 samples from 2851 text units in 1083 documents.\n"
     ]
    }
   ],
   "source": [
    "from benchmark_qed.autod.sampler.sample_gen import acreate_clustered_sample\n",
    "\n",
    "clustered_sample = await acreate_clustered_sample(\n",
    "    input_path=INPUT_DATA_PATH,\n",
    "    output_path=OUTPUT_DATA_PATH,\n",
    "    text_embedder=text_embedder,\n",
    "    num_clusters=NUM_CLUSTERS,\n",
    "    num_samples_per_cluster=NUM_SAMPLES_PER_CLUSTER,\n",
    "    input_type=\"json\",\n",
    "    text_tag=TEXT_COLUMN,\n",
    "    metadata_tags=METADATA_COLUMNS,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    file_encoding=FILE_ENCODING,\n",
    "    token_encoding=ENCODING_MODEL,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "print(\n",
    "    f\"Sampled {len(clustered_sample.sample_texts)} samples from {len(clustered_sample.text_units)} text units in {len(clustered_sample.documents)} documents.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ba32d",
   "metadata": {},
   "source": [
    "Data Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba51f175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Processing clusters 0 to 8 of 10 clusters...\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: What specific type of cancer was publicly announced as diagnosed in King Charles III during his treatment in early 2024 in the United Kingdom?. Intra-inter Similarity: 5.1593895592740635. Reference Coverage: 0.2\n",
      " 12%|█▎        | 1/8 [00:25<02:56, 25.16s/it]INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: What factors have contributed to the decline of pharmacies in West Virginia, and what measures are being considered to improve access to prescription medications in rural communities?. Intra-inter Similarity: 1.7171281526773072. Reference Coverage: 0.2\n",
      " 25%|██▌       | 2/8 [00:29<01:15, 12.64s/it]INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: What are the main environmental, social, and health-related factors contributing to the recent increase and geographic spread of malaria and dengue in Africa, Latin America, and other affected regions during 2022 to 2024?. Intra-inter Similarity: 2.9080106708722457. Reference Coverage: 0.8\n",
      " 38%|███▊      | 3/8 [00:37<00:53, 10.69s/it]INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: What are the health and safety risks of administering sedatives to individuals restrained by police in the United States between 2012 and 2021?. Intra-inter Similarity: 3.396842747208912. Reference Coverage: 0.4\n",
      " 50%|█████     | 4/8 [00:40<00:31,  7.79s/it]INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: What changes did the Oregon Legislature implement in 2024 regarding the criminalization of small-amount drug possession, and what were the reasons behind this legislative shift?. Intra-inter Similarity: 3.2008394056278786. Reference Coverage: 0.4\n",
      " 62%|██████▎   | 5/8 [00:41<00:16,  5.42s/it]INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In the context of Mississippi's political and health landscape in April 2024, what are the primary reasons that Mississippi lawmakers have historically opposed expanding Medicaid under the Affordable Care Act?. Intra-inter Similarity: 2.4872959472198377. Reference Coverage: 0.5\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: What are the recent judicial decisions in Montana and Arizona regarding abortion restrictions in the context of the post-Roe v. Wade legal landscape in the United States as of March 2024?. Intra-inter Similarity: 2.3208964777420484. Reference Coverage: 0.2\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: Given the political and health context in Mississippi in April 2024, why have the state's lawmakers historically opposed Medicaid expansion under the Affordable Care Act?. Intra-inter Similarity: 2.4301773456349394. Reference Coverage: 0.8\n",
      " 75%|███████▌  | 6/8 [00:52<00:14,  7.19s/it]INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In 2024, which U.S. states have enacted or proposed laws that restrict or ban gender-affirming healthcare for transgender minors, considering the legislative actions led by Republican-controlled state legislatures?. Intra-inter Similarity: 2.5071777618629767. Reference Coverage: 0.5\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In the context of 2024 U.S. state-level efforts following Roe v. Wade's overturn, how are Maine and Colorado working to protect or enshrine abortion rights within their state constitutions?. Intra-inter Similarity: 2.3346088412951755. Reference Coverage: 0.2\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: What are the recent legal disputes and challenges faced by Missouri and Texas regarding abortion laws and healthcare providers in 2024?. Intra-inter Similarity: 2.044322074074524. Reference Coverage: 0.2\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In 2024, which specific medical treatments such as surgeries, hormone therapies, or puberty blockers are being targeted by laws in U.S. states that restrict gender-affirming care for minors?. Intra-inter Similarity: 2.5169290792663666. Reference Coverage: 0.6\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In 2024, what is the stance of major medical organizations like the American Medical Association and the American Academy of Pediatrics regarding laws that restrict or ban gender-affirming care for transgender youth in the United States?. Intra-inter Similarity: 2.2989487610906014. Reference Coverage: 0.2\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: What specific legislative proposals or laws have Tennessee and South Dakota enacted or advanced in 2024 to restrict or regulate abortion access?. Intra-inter Similarity: 2.2193388496208706. Reference Coverage: 0.2\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In 2024, what legal actions, court rulings, or court decisions have affected the enforcement of laws restricting gender-affirming care for transgender individuals in the United States?. Intra-inter Similarity: 2.2918120380466718. Reference Coverage: 0.1\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: What is the impact of Arizona's revival of the 1864 law criminalizing abortion in 2024, and how does it relate to recent court rulings on abortion enforcement in the state?. Intra-inter Similarity: 2.278446600573053. Reference Coverage: 0.2\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In 2024, which U.S. states are considering or have enacted restrictions on gender-affirming healthcare for adults, including bans on surgeries and hormone treatments?. Intra-inter Similarity: 2.506435539510559. Reference Coverage: 0.3\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: How are the 2024 U.S. presidential candidates, including Joe Biden and Donald Trump, influencing the national abortion debate during the election cycle?. Intra-inter Similarity: 2.0400583648806876. Reference Coverage: 0.1\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In 2024, what are the primary arguments presented by supporters and opponents of laws restricting gender-affirming care for transgender youth and adults in the United States?. Intra-inter Similarity: 2.3078932419694542. Reference Coverage: 0.4\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: What are the potential impacts of 2024 federal court rulings on medication abortion drugs and emergency care requirements on nationwide abortion access?. Intra-inter Similarity: 2.0406203861173156. Reference Coverage: 0.4\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: What legal and legislative actions are Alabama and Idaho taking in 2024 concerning embryo rights and in vitro fertilization services?. Intra-inter Similarity: 2.1828137083468917. Reference Coverage: 0.2\n",
      " 88%|████████▊ | 7/8 [01:25<00:15, 15.67s/it]INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In 2024, what reasons do U.S. lawmakers give to justify restricting or banning gender-affirming care for minors and adults?. Intra-inter Similarity: 2.2522436757651727. Reference Coverage: 0.4\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In 2024, what do advocates believe will be the mental health consequences for transgender individuals if laws restricting gender-affirming care are enacted in the United States?. Intra-inter Similarity: 2.2266091934776475. Reference Coverage: 0.3\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In 2024, which U.S. states are considering or have enacted laws that restrict transgender youth's access to school pronouns, sports, or bathrooms?. Intra-inter Similarity: 2.578645615168592. Reference Coverage: 0.7\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In 2024, how does partisan politics influence the legislative process and enactment of laws restricting gender-affirming care for transgender individuals in the United States?. Intra-inter Similarity: 2.356725895529357. Reference Coverage: 0.6\n",
      "100%|██████████| 8/8 [01:44<00:00, 13.07s/it]\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Processing clusters 8 to 10 of 10 clusters...\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: Given the February 2023 train derailment in East Palestine, Ohio, what did the National Transportation Safety Board determine was the likely cause of the incident?. Intra-inter Similarity: 3.5668542255574556. Reference Coverage: 0.2\n",
      " 50%|█████     | 1/2 [00:16<00:16, 16.01s/it]INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Question: In the context of the 2022 overturning of Roe v. Wade by the U.S. Supreme Court, what role did Donald Trump play in enabling this decision through his judicial nominations, and how is this viewed by Biden and Harris?. Intra-inter Similarity: 2.7744299669704224. Reference Coverage: 0.2\n",
      "100%|██████████| 2/2 [00:18<00:00,  9.18s/it]\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.local_question_gen:Generated 27 candidate questions from 10 clusters.\n",
      "INFO:benchmark_qed.autod.sampler.clustering.kmeans:Cluster sizes: min=3, max=24, mean=13.5\n",
      "INFO:benchmark_qed.autoq.sampler.question_sampler:Selected 2 questions from 27 candidates.\n"
     ]
    }
   ],
   "source": [
    "from benchmark_qed.autoq.question_gen.data_questions.local_question_gen import (\n",
    "    DataLocalQuestionGen,\n",
    ")\n",
    "\n",
    "# load clustered text sample (result from the data sampling step)\n",
    "# If you have previously run the data sampling step, you can load the sample from disk instead of re-running the data sampling step as the below example.\n",
    "# Otherwise, you can use clustered_sample.sample_texts directly\n",
    "sample_texts_df = pd.read_parquet(f\"{OUTPUT_DATA_PATH}/sample_texts.parquet\")\n",
    "sample_texts = load_text_units(df=sample_texts_df)\n",
    "\n",
    "data_local_generator = DataLocalQuestionGen(\n",
    "    llm=llm,\n",
    "    text_embedder=text_embedder,\n",
    "    text_units=sample_texts,\n",
    "    concurrent_coroutines=CONCURRENT_REQUESTS,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "data_local_question_results = await data_local_generator.agenerate(\n",
    "    num_questions=NUM_QUESTIONS,\n",
    "    oversample_factor=OVERSAMPLE_FACTOR,\n",
    ")\n",
    "\n",
    "# save both candidate questions and the final selected questions\n",
    "save_questions(\n",
    "    data_local_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_local_questions/\",\n",
    "    \"selected_questions\",\n",
    ")\n",
    "save_questions(\n",
    "    data_local_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_local_questions/\",\n",
    "    \"selected_questions_text\",\n",
    "    question_text_only=True,\n",
    ")\n",
    "save_questions(\n",
    "    data_local_question_results.candidate_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_local_questions/\",\n",
    "    \"candidate_questions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2996f",
   "metadata": {},
   "source": [
    "Data Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52d97627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:benchmark_qed.autoq.question_gen.data_questions.global_question_gen:Number of initial categories: 97\n",
      "Number of valid candidate categories (i.e. categories with more than one input question): 12\n",
      "Number of questions to generate per candidate category: 1\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.global_question_gen:Processing categories 0 to 8 of 12 categories...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27 candidate local questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:51<00:11,  5.97s/it]WARNING:benchmark_qed.autoq.question_gen.data_questions.claim_extractor.local_claim_extractor:All extracted sources are not in the context records: ['1288']\n",
      "WARNING:benchmark_qed.autoq.question_gen.data_questions.claim_extractor.local_claim_extractor:All extracted sources are not in the context records: ['1288']\n",
      "ERROR:benchmark_qed.autoq.question_gen.data_questions.global_question_gen:Exception occurred while generating questions for category: judicial rulings\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shash\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\benchmark_qed\\autoq\\question_gen\\data_questions\\global_question_gen.py\", line 243, in _agenerate_single_chain\n",
      "    await self.claim_extractor.aextract_claims(\n",
      "  File \"c:\\Users\\shash\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\benchmark_qed\\autoq\\question_gen\\data_questions\\claim_extractor\\global_claim_extractor.py\", line 66, in aextract_claims\n",
      "    claims = [\n",
      "             ^\n",
      "  File \"c:\\Users\\shash\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\benchmark_qed\\autoq\\question_gen\\data_questions\\claim_extractor\\global_claim_extractor.py\", line 70, in <listcomp>\n",
      "    if claim[\"statement\"] != \"\" and len(claim[\"source_ids\"]) > 0\n",
      "                                        ~~~~~^^^^^^^^^^^^^^\n",
      "KeyError: 'source_ids'\n",
      " 88%|████████▊ | 7/8 [00:52<00:04,  4.48s/it]WARNING:benchmark_qed.autoq.question_gen.data_questions.claim_extractor.local_claim_extractor:All extracted sources are not in the context records: ['1479']\n",
      "WARNING:benchmark_qed.autoq.question_gen.data_questions.claim_extractor.local_claim_extractor:All extracted sources are not in the context records: ['1479']\n",
      "WARNING:benchmark_qed.autoq.question_gen.data_questions.claim_extractor.local_claim_extractor:All extracted sources are not in the context records: ['1479']\n",
      "ERROR:benchmark_qed.autoq.question_gen.data_questions.global_question_gen:Exception occurred while generating questions for category: public health\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shash\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\benchmark_qed\\autoq\\question_gen\\data_questions\\global_question_gen.py\", line 243, in _agenerate_single_chain\n",
      "    await self.claim_extractor.aextract_claims(\n",
      "  File \"c:\\Users\\shash\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\benchmark_qed\\autoq\\question_gen\\data_questions\\claim_extractor\\global_claim_extractor.py\", line 66, in aextract_claims\n",
      "    claims = [\n",
      "             ^\n",
      "  File \"c:\\Users\\shash\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\benchmark_qed\\autoq\\question_gen\\data_questions\\claim_extractor\\global_claim_extractor.py\", line 70, in <listcomp>\n",
      "    if claim[\"statement\"] != \"\" and len(claim[\"source_ids\"]) > 0\n",
      "                                        ~~~~~^^^^^^^^^^^^^^\n",
      "KeyError: 'source_ids'\n",
      "100%|██████████| 8/8 [01:10<00:00,  8.83s/it]\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.global_question_gen:Processing categories 8 to 12 of 12 categories...\n",
      "100%|██████████| 4/4 [00:18<00:00,  4.66s/it]\n",
      "INFO:benchmark_qed.autoq.question_gen.data_questions.global_question_gen:Generated 10 candidate questions from 27 local questions.\n",
      "INFO:benchmark_qed.autod.sampler.clustering.kmeans:Cluster sizes: min=5, max=5, mean=5.0\n",
      "INFO:benchmark_qed.autoq.sampler.question_sampler:Selected 2 questions from 10 candidates.\n"
     ]
    }
   ],
   "source": [
    "from benchmark_qed.autoq.question_gen.data_questions.global_question_gen import (\n",
    "    DataGlobalQuestionGen,\n",
    ")\n",
    "\n",
    "# Load candidate questions (result from the data local question generation step)\n",
    "# Please note that we load all the candidate local questions (not just the selected ones) as that gives us a bigger pool of local questions to aggregate from.\n",
    "# If you have previously run the data local question generation step, you can load the candidate questions from disk instead of re-running the data local question generation step as the below example.\n",
    "# Otherwise, you can use data_local_question_results.candidate_questions directly\n",
    "local_questions = load_questions(\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_local_questions/candidate_questions.json\"\n",
    ")\n",
    "print(f\"Loaded {len(local_questions)} candidate local questions.\")\n",
    "\n",
    "data_global_generator = DataGlobalQuestionGen(\n",
    "    llm=llm,\n",
    "    text_embedder=text_embedder,\n",
    "    local_questions=local_questions,\n",
    "    concurrent_coroutines=CONCURRENT_REQUESTS,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "data_global_question_results = await data_global_generator.agenerate(\n",
    "    num_questions=NUM_QUESTIONS,\n",
    "    oversample_factor=OVERSAMPLE_FACTOR,\n",
    ")\n",
    "\n",
    "# save both candidate questions and the final selected questions\n",
    "save_questions(\n",
    "    data_global_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_global_questions/\",\n",
    "    \"selected_questions\",\n",
    ")\n",
    "save_questions(\n",
    "    data_global_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_global_questions/\",\n",
    "    \"selected_questions_text\",\n",
    "    question_text_only=True,\n",
    ")\n",
    "save_questions(\n",
    "    data_global_question_results.candidate_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_global_questions/\",\n",
    "    \"candidate_questions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "715d832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.activity_context_gen:Generating dataset summary from 10 representative texts...\n",
      "INFO:benchmark_qed.autod.summarization.global_summarizer:Generating 1 map responses...\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.26s/it]\n",
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.activity_context_gen:Dataset summary: The dataset primarily draws from news articles and legislative reports, focusing on legal, political, health, and environmental issues. Its prominent theme centers on reproductive rights and related legislative and judicial developments, including the impact of major court rulings and state-level restrictions, as well as social and political debates surrounding transgender healthcare and abortion policies. Other key topics include public health concerns such as disease control, environmental health impacts from incidents like train derailments, and healthcare policy debates on drug pricing, Medicaid expansion, and vaccine requirements. Additionally, the dataset covers controversies in law enforcement practices, environmental contamination, and health updates on notable figures, all interconnected through themes of policy, ethics, and societal well-being.\n",
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.activity_context_gen:Generated 20 tasks.\n",
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.activity_context_gen:Extracting entities for 20 tasks...\n",
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\u001b[AINFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\n",
      "\u001b[A\u001b[AINFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[AINFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[AINFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[AINFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.56s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.78s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.75s/it]\n",
      "\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.91s/it]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.47s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.87s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.33s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.94s/it]\n",
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\u001b[AINFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\n",
      "\u001b[A\u001b[AINFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[AINFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[AINFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[AINFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.70s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.28s/it]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.86s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.08s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.18s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.40s/it]\n",
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\u001b[AINFO:benchmark_qed.autoq.question_gen.activity_questions.context_gen.entity_extractor:Generating 1 map responses...\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.79s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.76s/it]\n",
      "\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.35s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.54s/it]\n"
     ]
    }
   ],
   "source": [
    "from benchmark_qed.autoq.question_gen.activity_questions.context_gen.activity_context_gen import (\n",
    "    ActivityContextGen,\n",
    ")\n",
    "\n",
    "# load clustered text sample (result from the data sampling step)\n",
    "# If you have previously run the data sampling step, you can load the sample from disk instead of re-running the data sampling step as the below example.\n",
    "# Otherwise, you can use clustered_sample.sample_texts directly\n",
    "sample_texts_df = pd.read_parquet(f\"{OUTPUT_DATA_PATH}/sample_texts.parquet\")\n",
    "sample_texts = load_text_units(\n",
    "    df=sample_texts_df, attributes_cols=[\"is_representative\"]\n",
    ")\n",
    "\n",
    "activity_generator = ActivityContextGen(\n",
    "    llm=llm,\n",
    "    text_embedder=text_embedder,\n",
    "    token_encoder=token_encoder,\n",
    "    text_units=sample_texts,\n",
    "    concurrent_coroutines=CONCURRENT_REQUESTS,\n",
    ")\n",
    "\n",
    "activity_context = await activity_generator.agenerate(\n",
    "    num_personas=NUM_PERSONAS,\n",
    "    num_tasks=NUM_TASKS_PER_PERSONA,\n",
    "    num_entities_per_task=NUM_ENTITIES_PER_TASK,\n",
    "    oversample_factor=OVERSAMPLE_FACTOR,\n",
    "    use_representative_samples_only=True,  # if True, we will only use a subset of representative samples from the clustered texts to generate activity context (for efficiency). If False, we will use all the samples in the clustered texts.\n",
    ")\n",
    "\n",
    "save_activity_context(activity_context, f\"{OUTPUT_QUESTIONS_PATH}/context/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c607cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 tasks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.local_question_gen:Generated 23 candidate questions\n",
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.local_question_gen:Number of questions per entity: 1\n",
      "Number of entities: 19\n",
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.local_question_gen:Retained 14 questions post-filtering by entity distribution\n",
      "INFO:benchmark_qed.autod.sampler.clustering.kmeans:Cluster sizes: min=4, max=10, mean=7.0\n",
      "INFO:benchmark_qed.autoq.sampler.question_sampler:Selected 2 questions from 14 candidates.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from benchmark_qed.autoq.data_model.activity import ActivityContext\n",
    "from benchmark_qed.autoq.question_gen.activity_questions.local_question_gen import (\n",
    "    ActivityLocalQuestionGen,\n",
    ")\n",
    "\n",
    "# load activity context (result from the activity context generation step)\n",
    "# If you have previously run the activity context generation step, you can load the context from disk instead of re-running the activity context generation step as the below example.\n",
    "activity_context = ActivityContext(\n",
    "    **json.loads(\n",
    "        Path(f\"{OUTPUT_QUESTIONS_PATH}/context/activity_context_full.json\").read_text()\n",
    "    )\n",
    ")\n",
    "print(f\"Loaded {len(activity_context.task_contexts)} tasks.\")\n",
    "\n",
    "activity_local_generator = ActivityLocalQuestionGen(\n",
    "    llm=llm,\n",
    "    text_embedder=text_embedder,\n",
    "    activity_context=activity_context,\n",
    "    concurrent_coroutines=CONCURRENT_REQUESTS,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "activity_local_question_results = await activity_local_generator.agenerate(\n",
    "    num_questions=NUM_QUESTIONS,\n",
    "    oversample_factor=OVERSAMPLE_FACTOR,\n",
    ")\n",
    "\n",
    "# save both candidate questions and the final selected questions\n",
    "save_questions(\n",
    "    activity_local_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/activity_local_questions/\",\n",
    "    \"selected_questions\",\n",
    ")\n",
    "save_questions(\n",
    "    activity_local_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/activity_local_questions/\",\n",
    "    \"selected_questions_text\",\n",
    "    question_text_only=True,\n",
    ")\n",
    "save_questions(\n",
    "    activity_local_question_results.candidate_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/activity_local_questions/\",\n",
    "    \"candidate_questions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc0ea3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 tasks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:benchmark_qed.autoq.question_gen.activity_questions.global_question_gen:Generated 20 candidate questions for 20 tasks\n",
      "INFO:benchmark_qed.autod.sampler.clustering.kmeans:Cluster sizes: min=5, max=15, mean=10.0\n",
      "INFO:benchmark_qed.autoq.sampler.question_sampler:Selected 2 questions from 20 candidates.\n"
     ]
    }
   ],
   "source": [
    "from benchmark_qed.autoq.question_gen.activity_questions.global_question_gen import (\n",
    "    ActivityGlobalQuestionGen,\n",
    ")\n",
    "\n",
    "# load activity context (result from the activity context generation step)\n",
    "# If you have previously run the activity context generation step, you can load the context from disk instead of re-running the activity context generation step as the below example.\n",
    "activity_context = ActivityContext(\n",
    "    **json.loads(\n",
    "        Path(f\"{OUTPUT_QUESTIONS_PATH}/context/activity_context_full.json\").read_text()\n",
    "    )\n",
    ")\n",
    "print(f\"Loaded {len(activity_context.task_contexts)} tasks.\")\n",
    "\n",
    "activity_global_generator = ActivityGlobalQuestionGen(\n",
    "    llm=llm,\n",
    "    text_embedder=text_embedder,\n",
    "    activity_context=activity_context,\n",
    "    concurrent_coroutines=CONCURRENT_REQUESTS,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "activity_global_question_results = await activity_global_generator.agenerate(\n",
    "    num_questions=NUM_QUESTIONS,\n",
    "    oversample_factor=OVERSAMPLE_FACTOR,\n",
    ")\n",
    "\n",
    "# save both candidate questions and the final selected questions\n",
    "save_questions(\n",
    "    activity_global_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/activity_global_questions/\",\n",
    "    \"selected_questions\",\n",
    ")\n",
    "save_questions(\n",
    "    activity_global_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/activity_global_questions/\",\n",
    "    \"selected_questions_text\",\n",
    "    question_text_only=True,\n",
    ")\n",
    "save_questions(\n",
    "    activity_global_question_results.candidate_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/activity_global_questions/\",\n",
    "    \"candidate_questions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0b72e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
